{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9102e82-921b-4919-8a32-63db7ace6bea",
   "metadata": {},
   "source": [
    "## Compare old IITM LPS_track with newer IITM LPS_output files to see if they have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea4db6d-4a83-434b-bec8-625c53ae198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa0a2ca-f110-470a-a6ed-5b2510aa927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea22bf-55a7-469f-9363-acb810c68d96",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51580b9b-ec1a-4786-8e21-5290e33b3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\".join([str(int(x)).zfill(2) for x in list(group.iloc[0][2:5])]) outputs something of the form\n",
    "#    yyyymmdd, e.g., '20070618'\n",
    "\n",
    "def create_id(x):\n",
    "    key = (\"\".join([str(int(elem)).zfill(2) for elem in list(x.iloc[0,2:5])]) # yyyymmdd\n",
    "           +\"-\"\n",
    "           +str(int(x.iloc[0,5])) # hh\n",
    "           +\"-\"\n",
    "           +\"-\".join([str(float(elem)) for elem in list(x.iloc[1][3:5])]) # lon-lat\n",
    "          )\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530763b8-1ca6-4346-9809-31df18a32331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is 1 empty column at the start of all rows in LPS_track files (it is where\n",
    "#    the 'start' in the 'start 41 2007 06 18 00' lines goes)\n",
    "\n",
    "# param: csv: a text file (of the form LPS_track_yyyymmdd.txt) supplied by Vishnu\n",
    "# param: column_names: a list of column names\n",
    "# returns: a dictionary whose keys are dates of storms and values are dataframes\n",
    "def read_vishnu_csv_to_dic(csv, column_names, data_name, keyIncludesHour=False, \n",
    "                           keyToCheckForHit=None, obsToCheckForHit=None,\n",
    "                           lonMin=50, lonMax=100, latMin=0, latMax = 30, yearMin=2007, yearMax=2018,\n",
    "                           monthMin=6, monthMax=9):\n",
    "    \n",
    "    file = csv\n",
    "    df = pd.read_csv(file, header=None, names=column_names, sep='\\t')\n",
    "    \n",
    "    # df[0] gets the first column, which is empty except for 'start'\n",
    "    # .isin(['start'] returns a boolean array, where it is True wherever there is 'start' and False\n",
    "    #    otherwise\n",
    "    # .cumsum() returns an array of numbers, that starts at the first True and takes the value 1 at\n",
    "    #    every index until the next True, after which it takes the value 2 until the next True, and\n",
    "    #    so on.\n",
    "    # so, the dataframe is grouped into the distinct storms, based on whenever there is a 'start' line.\n",
    "    groups = df[0].isin(['start']).cumsum()\n",
    "    \n",
    "    # group.iloc[1:, 1:] gets rid of the first row (with 'start') and the first column (that is empty\n",
    "    #    in all the rows except for in the 'start' row\n",
    "    # so, in dic_of_dfs, the keys are the unique IDs based on genesis time and position\n",
    "    #    (like '20070618-12-87.5-17.5') and the values are dataframes\n",
    "    dic_of_dfs = {create_id(group): \n",
    "                  pd.concat([group.iloc[1:, 1:],\n",
    "                             pd.Series(create_id(group),\n",
    "                                       index=group.iloc[1:].index).rename(data_name+' key')], \n",
    "                            axis=1) \n",
    "                  for name, group in df.groupby(groups)\n",
    "                  if (lonMin <= group.iloc[1, 3] <= lonMax\n",
    "                      and latMin <= group.iloc[1, 4] <= latMax\n",
    "                      and yearMin <= group.iloc[0, 2] <= yearMax\n",
    "                      and monthMin <= group.iloc[0, 3] <= monthMax)}\n",
    "        # the if condition filters for storms that are over India,\n",
    "        #    and between 2007 and 2018,\n",
    "        #    and in the Indian summer monsoon season (JJAS: June, July, August, September)\n",
    "        # for 'over India', need longitude between 50 and 100 degrees, and latitude \n",
    "        #    between 5 and 30 degrees.\n",
    "        \n",
    "        # for reference:\n",
    "        # group.iloc[1:]['Longitude'].between(lonMin, lonMax).all() \n",
    "        # and group.iloc[1:]['Latitude'].between(latMin, latMax).all()\n",
    "    \n",
    "    return dic_of_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6b07cb-1e5f-4719-a3e0-c8239a588947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: df: a pandas dataframe containing columns titled 'Year', 'Month', 'Day', 'Hour'\n",
    "# returns: a datafram identical to df, except containing an extra column 'date'\n",
    "def add_datetime_to_df(df):\n",
    "    \n",
    "    # the .to_datetime method relies on the columns having sensible names 'Year', 'Month', etc.\n",
    "    df['date'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour']])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# param: dic: a dictionary whose values are dataframes\n",
    "# returns: a dictionary with all the dataframes having an extra 'date' column\n",
    "def add_datetime_to_dic(dic):\n",
    "    \n",
    "    return {key: add_datetime_to_df(value) for key, value in dic.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aafcf2d-fe29-4c2a-b804-5375b157949f",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62be52d5-b61d-4946-9dff-991e8097322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IITM_new = {}\n",
    "nempty_iitm = 0\n",
    "ntotal_iitm = 0\n",
    "empty_dates_new = []\n",
    "\n",
    "column_names = [0, 'XGridCentre', 'YGridCentre', 'Longitude', 'Latitude', 'MinSF', 'MinSLP', 'AvgRH', \n",
    "                'MaxSfcGeoPt', 'MaxSfcWind', 'minmslix', 'Year', 'Month', 'Day', 'Hour']\n",
    "\n",
    "folder_path = '/global/homes/s/salilg/pandasNewCode/tempest_extremes/iitm_output_files_salil/'\n",
    "for outputfile in sorted(os.listdir(folder_path)):\n",
    "    if outputfile.startswith('LPS_dslp_output'):\n",
    "        \n",
    "        date = outputfile[16:24]\n",
    "        #all IITM file dates are observed date + 1; so moving back one day here to match with ERA5 file dates\n",
    "        # date = changeIITMFileDate(fileDate)\n",
    "        \n",
    "        file = folder_path + outputfile\n",
    "        \n",
    "        if os.stat(file).st_size == 0:\n",
    "            nempty_iitm += 1\n",
    "            empty_dates_new.append(date)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            dic = add_datetime_to_dic(\n",
    "                read_vishnu_csv_to_dic(file, column_names, 'IITM')\n",
    "                                            )\n",
    "            IITM_new[date] = dic # this risks skipping dates that are duplicated, but\n",
    "            # not important for now\n",
    "            \n",
    "            # load_model_data(date, possible_model_tracks, IITM, ERA5, 'ERA5')\n",
    "            \n",
    "        ntotal_iitm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cea2557-c0f5-4287-9cba-ae651dad0131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nempty_iitm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccec5602-8e3a-4b24-a4e5-66161ab57948",
   "metadata": {},
   "outputs": [],
   "source": [
    "IITM_old = {}\n",
    "# IITM_multiple_hits = {}\n",
    "\n",
    "column_names = [0, 'XGridCentre', 'YGridCentre', 'Longitude', 'Latitude', \n",
    "                    'MinSF', 'MinSLP', 'PressureDrop', 'MaxSfcWind', 'AvgRH', 'MaxSfcGeoPt', 'Year', \n",
    "                    'Month', 'Day', 'Hour']\n",
    "\n",
    "for outputfile in sorted(os.listdir('/global/cscratch1/sd/vishnus/GFS_IITM/2007_2018_newgp/')):\n",
    "    if (outputfile.startswith('LPS_dslp_track_')): # and \n",
    "        # os.stat('/global/cscratch1/sd/vishnus/GFS_IITM/2007_2018_newgp/' + outputfile).st_size > 0):\n",
    "        \n",
    "        date = outputfile[15:23]\n",
    "        # all IITM file dates are observed date + 1; so moving back one day here\n",
    "        # date = changeIITMFileDate(fileDate)\n",
    "        \n",
    "        # if date in common_hits:\n",
    "        \n",
    "        file = '/global/cscratch1/sd/vishnus/GFS_IITM/2007_2018_newgp/' + outputfile\n",
    "            \n",
    "        dic = add_datetime_to_dic(\n",
    "            read_vishnu_csv_to_dic(file, column_names, 'IITM')\n",
    "                                        )\n",
    "        IITM_old[date] = dic # this risks skipping dates that are duplicated, but\n",
    "        # not important for now\n",
    "\n",
    "        # load_model_data(date, possible_model_tracks, IITM, ERA5, 'ERA5')\n",
    "            \n",
    "            # get_num_multiple_hits(date, possible_model_tracks, ERA5, IITM_multiple_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07ee758f-d993-4ec8-81b2-8e59694dec97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 102)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(IITM_new), len(IITM_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923fd139-2fcf-4cad-9906-7720c12eae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common = [x for x in IITM_old.keys() if x in IITM_new.keys()]\n",
    "len(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515d6eba-339f-4d89-a2d6-c1361cabfd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 14)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_from_new = [key for key in IITM_old.keys() if key not in IITM_new.keys()]\n",
    "len(empty_dates_new), len(missing_from_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b9dd36-1f2b-43ca-a0a7-d8d56706b4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20110603',\n",
       " '20110614',\n",
       " '20110705',\n",
       " '20110719',\n",
       " '20110808',\n",
       " '20110814',\n",
       " '20110819',\n",
       " '20110830',\n",
       " '20110905',\n",
       " '20110919']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it looks like some of the dates (all 2011) in the new candidate files are off by one day.\n",
    "# for example, there's a 20110602, 20110613, 20110704, 20110718, 20110807, 20110813, 20110818,\n",
    "# 20110829, 20110904, 20110918\n",
    "# but there are also some 2011 dates that are included (20110607, etc.)\n",
    "\n",
    "[x for x in missing_from_new if x not in empty_dates_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6226222-8870-4feb-b379-5ab86c860d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0424db0b-cba7-4682-adb3-6ef534dd64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "IITM_new_same_length = {}\n",
    "for date in IITM_new:\n",
    "    if date in IITM_old:\n",
    "        IITM_new_same_length[date] = IITM_new[date]\n",
    "    elif date in empty_dates_new:\n",
    "        continue\n",
    "    else:\n",
    "        new = datetime(int(date[:4]), int(date[4:6]), int(date[6:8])) + timedelta(1)\n",
    "        newdate = str(new.year).zfill(4) + str(new.month).zfill(2) + str(new.day).zfill(2)\n",
    "        if newdate in IITM_old:\n",
    "            IITM_new_same_length[newdate] = IITM_new[date]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2341a643-9707-4cf1-a388-b7359219b77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(IITM_new_same_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437ff5f-f32d-4f7f-bcaf-f809ff43f213",
   "metadata": {},
   "source": [
    "## compare all the common dates and see if dataframes are exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a259f7fa-7f3c-465f-a4cf-0e1020b25ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 0 115 0\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "k = 0\n",
    "diff, same = 0, 0\n",
    "for date in common:\n",
    "    # check if, for a given date, both files have the same number of tracks\n",
    "    if len(IITM_new[date]) != len(IITM_old[date]):\n",
    "        n += 1\n",
    "    else:\n",
    "        for track_new, track_old in zip(IITM_new[date], IITM_old[date]):\n",
    "            # if, for a given date, both files have the same number of tracks, then\n",
    "            # the sub-dictionaries (IITM_new[date] and IITM_old[date]) should be of the \n",
    "            # same length, and moreover the keys for each track should be the same\n",
    "            if track_new != track_old: # compare the keys\n",
    "                k += 1\n",
    "            else:\n",
    "                # print(date, track_new, track_old)\n",
    "                df_new = IITM_new[date][track_new].drop(columns=['MaxSfcGeoPt',\n",
    "                                                                 'minmslix']).sort_index(axis=1)\n",
    "                df_old = IITM_old[date][track_old].drop(columns=['MaxSfcGeoPt',\n",
    "                                                                 'PressureDrop']).sort_index(axis=1)\n",
    "                if df_new.equals(df_old):\n",
    "                    same += 1\n",
    "                else:\n",
    "                    diff += 1\n",
    "print(n, k, same, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e8c781b-1b5a-447d-bd50-fdeb3357152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 4 115 0\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "m = 0\n",
    "diff, same = 0, 0\n",
    "for date in [x for x in IITM_new_same_length.keys() if x in IITM_old.keys()]:\n",
    "    # check if, for a given date, both files have the same number of tracks\n",
    "    if len(IITM_new_same_length[date]) != len(IITM_old[date]):\n",
    "        n += 1\n",
    "    else:\n",
    "        k = 0\n",
    "        for track_new, track_old in zip(IITM_new_same_length[date], IITM_old[date]):\n",
    "            # if, for a given date, both files have the same number of tracks, then\n",
    "            # the sub-dictionaries (IITM_new[date] and IITM_old[date]) should be of the \n",
    "            # same length, and moreover the keys for each track should be the same\n",
    "            if track_new != track_old: # compare the keys\n",
    "                k += 1\n",
    "            else:\n",
    "                # print(date, track_new, track_old)\n",
    "                df_new = IITM_new_same_length[date][track_new].drop(columns=['MaxSfcGeoPt',\n",
    "                                                                 'minmslix']).sort_index(axis=1)\n",
    "                df_old = IITM_old[date][track_old].drop(columns=['MaxSfcGeoPt',\n",
    "                                                                 'PressureDrop']).sort_index(axis=1)\n",
    "                if df_new.equals(df_old):\n",
    "                    same += 1\n",
    "                else:\n",
    "                    diff += 1\n",
    "        if k > 0:\n",
    "            m += 1\n",
    "print(n, m, same, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d94b0dd9-b4aa-427b-a52f-6ec40e6e095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = IITM_new['20180916']['20180916-0-88.75-14.25']\n",
    "df_old = IITM_old['20180916']['20180916-0-88.75-14.25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71f41e8a-fbc2-44bb-9ffd-9bc97f504c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20070624-0-66.875-20.5', '20070629-3-87.75-21.625'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from LPS_output file, without nodefileeditor\n",
    "IITM_new['20070624'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7fdee3-9777-4a47-9df8-1648e83af9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20070624-0-66.875-20.5', '20070626-9-85.0-17.375', '20070629-3-87.75-21.625'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from LPS_track file, after nodefileeditor\n",
    "IITM_old['20070624'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50227822-6c87-471c-8068-78299e099c4b",
   "metadata": {},
   "source": [
    "## run tempestextremes on ECMWF again with the IITM conditions (minelength 9, maxgap 4) and see if the ECMWF output files are different from the existing track files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc27e1b7-599d-4505-8713-a23c2434d3df",
   "metadata": {},
   "source": [
    "### rather, we already have old output files. So can also compare the new ones against those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54e2d10b-c326-4a2c-8222-1712941d8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECMWF_old_track = {}\n",
    "\n",
    "column_names = [0, 'XGridCentre', 'YGridCentre', 'Longitude', 'Latitude', \n",
    "                    'MinSF', 'MinSLP', 'PressureDrop', 'MaxSfcWind', 'AvgRH', 'MaxSfcGeoPt', 'Year', \n",
    "                    'Month', 'Day', 'Hour']\n",
    "\n",
    "years = ['2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "\n",
    "for year in years:\n",
    "    for outputfile in sorted(os.listdir('/global/project/projectdirs/m3310/salilg/' + year)):\n",
    "        if (outputfile.startswith('LPS_track_')):\n",
    "\n",
    "            date = outputfile[10:18]\n",
    "\n",
    "            file = '/global/project/projectdirs/m3310/salilg/' + year + '/' + outputfile\n",
    "\n",
    "            dic = add_datetime_to_dic(\n",
    "                read_vishnu_csv_to_dic(file, column_names, 'ECMWF')\n",
    "                                            )\n",
    "            ECMWF_old_track[date] = dic # this risks skipping dates that are duplicated, but\n",
    "            # not important for now\n",
    "\n",
    "            # load_model_data(date, possible_model_tracks, IITM, ERA5, 'ERA5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e582a54-306a-46f5-871b-5f2f067f567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECMWF_old_output = {}\n",
    "\n",
    "column_names = [0, 'XGridCentre', 'YGridCentre', 'Longitude', 'Latitude', 'MinSF', 'MinSLP', 'AvgRH', \n",
    "                'MaxSfcGeoPt', 'MaxSfcWind', 'minmslix', 'Year', 'Month', 'Day', 'Hour']\n",
    "\n",
    "years = ['2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "\n",
    "for year in years:\n",
    "    for outputfile in sorted(os.listdir('/global/project/projectdirs/m3310/salilg/' + year)):\n",
    "        if (outputfile.startswith('LPS_output_')):\n",
    "\n",
    "            date = outputfile[11:19]\n",
    "\n",
    "            file = '/global/project/projectdirs/m3310/salilg/' + year + '/' + outputfile\n",
    "\n",
    "            dic = add_datetime_to_dic(\n",
    "                read_vishnu_csv_to_dic(file, column_names, 'ECMWF')\n",
    "                                            )\n",
    "            ECMWF_old_output[date] = dic # this risks skipping dates that are duplicated, but\n",
    "            # not important for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58d0274d-e61a-4c29-b8a0-5e9a2b22b122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 135)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ECMWF_old_track), len(ECMWF_old_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a59e78cb-03a4-4772-afa6-ee12ea577477",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECMWF_new_output = {}\n",
    "\n",
    "column_names = [0, 'XGridCentre', 'YGridCentre', 'Longitude', 'Latitude', 'MinSF', 'MinSLP', 'AvgRH', \n",
    "                'MaxSfcGeoPt', 'MaxSfcWind', 'minmslix', 'Year', 'Month', 'Day', 'Hour']\n",
    "\n",
    "\n",
    "for outputfile in sorted(os.listdir('/global/homes/s/salilg/pandasNewCode/tempest_extremes/\\\n",
    "ecmwf_test_output_files_salil/')):\n",
    "    if (outputfile.startswith('LPS_output_')):\n",
    "\n",
    "        date = outputfile[11:19]\n",
    "\n",
    "        file = '/global/homes/s/salilg/pandasNewCode/tempest_extremes/\\\n",
    "ecmwf_test_output_files_salil/' + outputfile\n",
    "\n",
    "        dic = add_datetime_to_dic(\n",
    "            read_vishnu_csv_to_dic(file, column_names, 'ECMWF')\n",
    "                                        )\n",
    "        ECMWF_new_output[date] = dic # this risks skipping dates that are duplicated, but\n",
    "        # not important for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef5cf5b7-e1e9-416f-ba55-7d9d093efde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this includes 2019 files. Only going till 2018 will get us 135 files.\n",
    "len(ECMWF_new_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce981195-0148-4f33-a5f1-648edd32cbd0",
   "metadata": {},
   "source": [
    "### compare the common dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1396723-1dcf-43f3-803e-fbd515fe3453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in ECMWF_new_output.keys() if x in ECMWF_old_track.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6a4166e-d35a-4418-8b45-7770289d69d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 255 0\n"
     ]
    }
   ],
   "source": [
    "# dic_1 must have output files\n",
    "# dic_2 must have track files\n",
    "dic_1 = ECMWF_old_output\n",
    "dic_2 = ECMWF_old_track\n",
    "\n",
    "n = 0\n",
    "m = 0\n",
    "diff, same = 0, 0\n",
    "for date in [x for x in dic_1.keys() if x in dic_2.keys()]:\n",
    "    # check if, for a given date, both files have the same number of tracks\n",
    "    if len(dic_1[date]) != len(dic_2[date]):\n",
    "        n += 1\n",
    "    else:\n",
    "        k = 0\n",
    "        for track_new, track_old in zip(dic_1[date], dic_2[date]):\n",
    "            # if, for a given date, both files have the same number of tracks, then\n",
    "            # the sub-dictionaries (IITM_new[date] and IITM_old[date]) should be of the \n",
    "            # same length, and moreover the keys for each track should be the same\n",
    "            if track_new != track_old: # compare the keys\n",
    "                k += 1\n",
    "            else:\n",
    "                # print(date, track_new, track_old)\n",
    "                df_new = dic_1[date][track_new].drop(columns=['minmslix']).sort_index(axis=1)\n",
    "                df_old = dic_2[date][track_old].drop(columns=['PressureDrop']).sort_index(axis=1)\n",
    "                if df_new.equals(df_old):\n",
    "                    same += 1\n",
    "                else:\n",
    "                    diff += 1\n",
    "        if k > 0:\n",
    "            m += 1\n",
    "print(n, m, same, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19058c5b-db9f-4445-82e1-22ef7696423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 5 6\n"
     ]
    }
   ],
   "source": [
    "# dic_1 must have output files\n",
    "# dic_2 must have track files\n",
    "dic_1 = ECMWF_new_output\n",
    "dic_2 = ECMWF_old_track\n",
    "\n",
    "n = 0\n",
    "m = 0\n",
    "diff_nsystems = 0\n",
    "for date in [x for x in dic_1.keys() if x in dic_2.keys()]:\n",
    "    # check if, for a given date, both files have the same number of tracks\n",
    "    if len(dic_1[date]) != len(dic_2[date]):\n",
    "        n += 1\n",
    "    else:\n",
    "        k = 0\n",
    "        same, diff = 0, 0\n",
    "        for track_new, track_old in zip(dic_1[date], dic_2[date]):\n",
    "            # if, for a given date, both files have the same number of tracks, then\n",
    "            # the sub-dictionaries (IITM_new[date] and IITM_old[date]) should be of the \n",
    "            # same length, and moreover the keys for each track should be the same\n",
    "            if track_new != track_old: # compare the keys\n",
    "                k += 1\n",
    "            else:\n",
    "                # print(date, track_new, track_old)\n",
    "                df_new = dic_1[date][track_new].drop(columns=['MaxSfcGeoPt',\n",
    "                                                                 'minmslix']).sort_index(axis=1)\n",
    "                df_old = dic_2[date][track_old].drop(columns=['MaxSfcGeoPt',\n",
    "                                                                 'PressureDrop']).sort_index(axis=1)\n",
    "                if df_new.equals(df_old):\n",
    "                    same += 1\n",
    "                else:\n",
    "                    diff += 1\n",
    "        if k > 0:\n",
    "            m += 1\n",
    "        if diff > 0:\n",
    "            diff_nsystems += 1\n",
    "            \n",
    "print(n, m, diff_nsystems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5febfcad-6dbf-453b-92b4-1b165ce0ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 5 100 6\n"
     ]
    }
   ],
   "source": [
    "# dic_1 must have output files\n",
    "# dic_2 must have track files\n",
    "dic_1 = ECMWF_new_output\n",
    "dic_2 = ECMWF_old_output\n",
    "\n",
    "n = 0\n",
    "m = 0\n",
    "diff, same = 0, 0\n",
    "for date in [x for x in dic_1.keys() if x in dic_2.keys()]:\n",
    "    # check if, for a given date, both files have the same number of tracks\n",
    "    if len(dic_1[date]) != len(dic_2[date]):\n",
    "        n += 1\n",
    "    else:\n",
    "        k = 0\n",
    "        for track_new, track_old in zip(dic_1[date], dic_2[date]):\n",
    "            # if, for a given date, both files have the same number of tracks, then\n",
    "            # the sub-dictionaries (IITM_new[date] and IITM_old[date]) should be of the \n",
    "            # same length, and moreover the keys for each track should be the same\n",
    "            if track_new != track_old: # compare the keys\n",
    "                k += 1\n",
    "            else:\n",
    "                # print(date, track_new, track_old)\n",
    "                df_new = dic_1[date][track_new].drop(columns=['MaxSfcGeoPt']).sort_index(axis=1)\n",
    "                df_old = dic_2[date][track_old].drop(columns=['MaxSfcGeoPt']).sort_index(axis=1)\n",
    "                if df_new.equals(df_old):\n",
    "                    same += 1\n",
    "                else:\n",
    "                    diff += 1\n",
    "        if k > 0:\n",
    "            m += 1\n",
    "print(n, m, same, diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1252d4-3614-4282-a064-210f60fc2444",
   "metadata": {},
   "source": [
    "## do some manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8e9450e-bd0e-4cc5-be2a-f36ccee28ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_new = IITM_new[common[80]]\n",
    "dic_old = IITM_old[common[80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cde9785a-41a6-4350-9761-424e7d7e2b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20180720-0-89.25-18.625'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4996b009-78e8-487b-9157-b7988e915315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20180720-0-89.25-18.625'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_old.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c9586fd-903d-48e3-b364-83636d2c6100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5656.589957416666"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([df['MaxSfcGeoPt'].mean() for df in dic_old.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbcd2fff-d849-42e2-9d59-c93adbb4c44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6430.825318541666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([df['MaxSfcGeoPt'].mean() for df in dic_new.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ebdca08-3c8e-4a6b-a6b3-a9caac0e735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5461.5027530916195"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.array([])\n",
    "for dic in IITM_new.values():\n",
    "    for df in dic.values():\n",
    "        total = np.concatenate((total, df['MaxSfcGeoPt'].to_numpy()))\n",
    "        \n",
    "np.mean(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ad8e05e-1834-4b0e-9abd-3ef62cf640cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5144.131446476719"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.array([])\n",
    "for dic in IITM_old.values():\n",
    "    for df in dic.values():\n",
    "        total = np.concatenate((total, df['MaxSfcGeoPt'].to_numpy()))\n",
    "        \n",
    "np.mean(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00eba41d-5dc8-4e15-9bab-2192cfdc3dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XGridCentre</th>\n",
       "      <th>YGridCentre</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>MinSF</th>\n",
       "      <th>MinSLP</th>\n",
       "      <th>PressureDrop</th>\n",
       "      <th>MaxSfcWind</th>\n",
       "      <th>AvgRH</th>\n",
       "      <th>MaxSfcGeoPt</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>IITM key</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>389</td>\n",
       "      <td>89.250</td>\n",
       "      <td>18.625</td>\n",
       "      <td>-22730310.0</td>\n",
       "      <td>99311.39</td>\n",
       "      <td>319.500</td>\n",
       "      <td>15.51504</td>\n",
       "      <td>91.67205</td>\n",
       "      <td>6.712623</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394</td>\n",
       "      <td>392</td>\n",
       "      <td>89.250</td>\n",
       "      <td>19.000</td>\n",
       "      <td>-23440790.0</td>\n",
       "      <td>99354.31</td>\n",
       "      <td>241.750</td>\n",
       "      <td>17.63548</td>\n",
       "      <td>91.78888</td>\n",
       "      <td>6.712623</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>393</td>\n",
       "      <td>397</td>\n",
       "      <td>89.125</td>\n",
       "      <td>19.625</td>\n",
       "      <td>-23337360.0</td>\n",
       "      <td>99333.03</td>\n",
       "      <td>183.000</td>\n",
       "      <td>17.67655</td>\n",
       "      <td>92.01437</td>\n",
       "      <td>10.525010</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>386</td>\n",
       "      <td>399</td>\n",
       "      <td>88.250</td>\n",
       "      <td>19.875</td>\n",
       "      <td>-23334120.0</td>\n",
       "      <td>99165.12</td>\n",
       "      <td>217.125</td>\n",
       "      <td>16.71610</td>\n",
       "      <td>90.89626</td>\n",
       "      <td>630.744700</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>384</td>\n",
       "      <td>401</td>\n",
       "      <td>88.000</td>\n",
       "      <td>20.125</td>\n",
       "      <td>-23989960.0</td>\n",
       "      <td>99158.81</td>\n",
       "      <td>147.250</td>\n",
       "      <td>19.52704</td>\n",
       "      <td>90.71742</td>\n",
       "      <td>2035.791000</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   XGridCentre  YGridCentre  Longitude  Latitude       MinSF    MinSLP  \\\n",
       "1          394          389     89.250    18.625 -22730310.0  99311.39   \n",
       "2          394          392     89.250    19.000 -23440790.0  99354.31   \n",
       "3          393          397     89.125    19.625 -23337360.0  99333.03   \n",
       "4          386          399     88.250    19.875 -23334120.0  99165.12   \n",
       "5          384          401     88.000    20.125 -23989960.0  99158.81   \n",
       "\n",
       "   PressureDrop  MaxSfcWind     AvgRH  MaxSfcGeoPt    Year  Month   Day  Hour  \\\n",
       "1       319.500    15.51504  91.67205     6.712623  2018.0    7.0  20.0   0.0   \n",
       "2       241.750    17.63548  91.78888     6.712623  2018.0    7.0  20.0   3.0   \n",
       "3       183.000    17.67655  92.01437    10.525010  2018.0    7.0  20.0   6.0   \n",
       "4       217.125    16.71610  90.89626   630.744700  2018.0    7.0  20.0   9.0   \n",
       "5       147.250    19.52704  90.71742  2035.791000  2018.0    7.0  20.0  12.0   \n",
       "\n",
       "                  IITM key                date  \n",
       "1  20180720-0-89.25-18.625 2018-07-20 00:00:00  \n",
       "2  20180720-0-89.25-18.625 2018-07-20 03:00:00  \n",
       "3  20180720-0-89.25-18.625 2018-07-20 06:00:00  \n",
       "4  20180720-0-89.25-18.625 2018-07-20 09:00:00  \n",
       "5  20180720-0-89.25-18.625 2018-07-20 12:00:00  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_old[list(dic_old.keys())[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a26ad12c-5cb1-46ce-bf16-95decaf9c999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XGridCentre</th>\n",
       "      <th>YGridCentre</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>MinSF</th>\n",
       "      <th>MinSLP</th>\n",
       "      <th>AvgRH</th>\n",
       "      <th>MaxSfcGeoPt</th>\n",
       "      <th>MaxSfcWind</th>\n",
       "      <th>minmslix</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>IITM key</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>389</td>\n",
       "      <td>89.250</td>\n",
       "      <td>18.625</td>\n",
       "      <td>-22730310.0</td>\n",
       "      <td>99311.39</td>\n",
       "      <td>91.67205</td>\n",
       "      <td>36.74033</td>\n",
       "      <td>15.51504</td>\n",
       "      <td>254871.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394</td>\n",
       "      <td>392</td>\n",
       "      <td>89.250</td>\n",
       "      <td>19.000</td>\n",
       "      <td>-23440790.0</td>\n",
       "      <td>99354.31</td>\n",
       "      <td>91.78888</td>\n",
       "      <td>36.74033</td>\n",
       "      <td>17.63548</td>\n",
       "      <td>256151.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>393</td>\n",
       "      <td>397</td>\n",
       "      <td>89.125</td>\n",
       "      <td>19.625</td>\n",
       "      <td>-23337360.0</td>\n",
       "      <td>99333.03</td>\n",
       "      <td>92.01437</td>\n",
       "      <td>36.74033</td>\n",
       "      <td>17.67655</td>\n",
       "      <td>257430.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>386</td>\n",
       "      <td>399</td>\n",
       "      <td>88.250</td>\n",
       "      <td>19.875</td>\n",
       "      <td>-23334120.0</td>\n",
       "      <td>99165.12</td>\n",
       "      <td>90.89626</td>\n",
       "      <td>259.42930</td>\n",
       "      <td>16.71610</td>\n",
       "      <td>258070.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>384</td>\n",
       "      <td>401</td>\n",
       "      <td>88.000</td>\n",
       "      <td>20.125</td>\n",
       "      <td>-23989960.0</td>\n",
       "      <td>99158.81</td>\n",
       "      <td>90.71742</td>\n",
       "      <td>1890.08700</td>\n",
       "      <td>19.52704</td>\n",
       "      <td>259349.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20180720-0-89.25-18.625</td>\n",
       "      <td>2018-07-20 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   XGridCentre  YGridCentre  Longitude  Latitude       MinSF    MinSLP  \\\n",
       "1          394          389     89.250    18.625 -22730310.0  99311.39   \n",
       "2          394          392     89.250    19.000 -23440790.0  99354.31   \n",
       "3          393          397     89.125    19.625 -23337360.0  99333.03   \n",
       "4          386          399     88.250    19.875 -23334120.0  99165.12   \n",
       "5          384          401     88.000    20.125 -23989960.0  99158.81   \n",
       "\n",
       "      AvgRH  MaxSfcGeoPt  MaxSfcWind  minmslix    Year  Month   Day  Hour  \\\n",
       "1  91.67205     36.74033    15.51504  254871.0  2018.0    7.0  20.0   0.0   \n",
       "2  91.78888     36.74033    17.63548  256151.0  2018.0    7.0  20.0   3.0   \n",
       "3  92.01437     36.74033    17.67655  257430.0  2018.0    7.0  20.0   6.0   \n",
       "4  90.89626    259.42930    16.71610  258070.0  2018.0    7.0  20.0   9.0   \n",
       "5  90.71742   1890.08700    19.52704  259349.0  2018.0    7.0  20.0  12.0   \n",
       "\n",
       "                  IITM key                date  \n",
       "1  20180720-0-89.25-18.625 2018-07-20 00:00:00  \n",
       "2  20180720-0-89.25-18.625 2018-07-20 03:00:00  \n",
       "3  20180720-0-89.25-18.625 2018-07-20 06:00:00  \n",
       "4  20180720-0-89.25-18.625 2018-07-20 09:00:00  \n",
       "5  20180720-0-89.25-18.625 2018-07-20 12:00:00  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_new[list(dic_new.keys())[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c618858-a0cd-4b25-9d99-d4b7f821876e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xarrayenv)",
   "language": "python",
   "name": "xarrayenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
